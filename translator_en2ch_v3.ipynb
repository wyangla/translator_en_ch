{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of translation  \n",
    "- English -> Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CentOS Linux release 7.6.1810 (Core) \r\n"
     ]
    }
   ],
   "source": [
    "!cat /etc/system-release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/shared/workspace/Dev/translate\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_bak\t\t       glove.6B.50d_gensim.txt\t   sgns.baidubaike.bigram-char\r\n",
      "enW2v_cleaned_weights  glove.6B.50d_gensim.txt.gz\r\n",
      "enW2v_idx2Lang\t       original\r\n"
     ]
    }
   ],
   "source": [
    "!ls Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### hypter parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import jieba\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import traceback\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "import os\n",
    "import urllib\n",
    "import json\n",
    "import requests\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetPath = r'/root/shared/workspace/Dev/translate/Dataset'\n",
    "enPath = os.path.join(datasetPath, r'original/en-zh/train.tags.en-zh.en')\n",
    "chPath = os.path.join(datasetPath, r'original/en-zh/train.tags.en-zh.zh')\n",
    "\n",
    "enW2VPath = os.path.join(datasetPath, r'glove.6B.50d_gensim.txt.gz')\n",
    "chW2VPath = os.path.join(datasetPath, r'sgns.baidubaike.bigram-char.bz2')\n",
    "\n",
    "'''model parameters'''\n",
    "\n",
    "maxSenLen = 20    # terms + one <tag>\n",
    "hiddenUnitNum = 64    # ref: 1, 2 1000 -> 300\n",
    "\n",
    "# enVocabularySize = 10000    # 51427 -> 10000\n",
    "# chVocabularySize = 4000    # 88537 -> 4000\n",
    "# embeddingSize = 50\n",
    "\n",
    "# make use of the trained embeddings\n",
    "enVocabularySize = 400000\n",
    "chVocabularySize = 4000\n",
    "enEmbSize = 50\n",
    "chEmbSize = 300\n",
    "\n",
    "energySize = 10\n",
    "\n",
    "batchSize = 100\n",
    "datasetSize = 200000    # not really the full size\n",
    "stepsPerEpoch = int(datasetSize / batchSize)\n",
    "\n",
    "# bufferSize = 30000\n",
    "# prefetchSize = 1000\n",
    "# seed = 1\n",
    "\n",
    "checkpointDir = r'/root/shared/workspace/Dev/translate/checkpoints'\n",
    "# checkpointPath = os.path.join(checkpointDir, 'translator.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from logging import getLogger, Formatter\n",
    "from logging import handlers, StreamHandler\n",
    "\n",
    "class Logger():\n",
    "    @classmethod\n",
    "    def get_logger(self, name = 'test', persist = False):\n",
    "        \n",
    "        lg = logging.root.manager.loggerDict.get(name)\n",
    "        if type(lg) == type(None):\n",
    "            lg = getLogger(name)    # ref: 23\n",
    "            lg.setLevel('DEBUG')\n",
    "\n",
    "            fmt = Formatter('%(name)s %(asctime)s [%(levelname)s] - %(filename)s:%(lineno)s %(message)s')\n",
    "\n",
    "            shandler = StreamHandler()\n",
    "            shandler.setFormatter(fmt)\n",
    "            lg.addHandler(shandler)\n",
    "\n",
    "            if persist:\n",
    "                fhandler = handlers.RotatingFileHandler('./' + name + '.log', mode = 'a', maxBytes = 1024 * 1024, backupCount = 3)\n",
    "                fhandler.setFormatter(fmt)\n",
    "                lg.addHandler(fhandler)\n",
    "        \n",
    "        return lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! curl -o en-zh.tgz https://wit3.fbk.eu/archive/2015-01//texts/en/zh/en-zh.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en-zh/\n",
      "en-zh/IWSLT15.TED.dev2010.en-zh.en.xml\n",
      "en-zh/IWSLT15.TED.dev2010.en-zh.zh.xml\n",
      "en-zh/IWSLT15.TED.tst2010.en-zh.en.xml\n",
      "en-zh/IWSLT15.TED.tst2010.en-zh.zh.xml\n",
      "en-zh/IWSLT15.TED.tst2011.en-zh.en.xml\n",
      "en-zh/IWSLT15.TED.tst2011.en-zh.zh.xml\n",
      "en-zh/IWSLT15.TED.tst2012.en-zh.en.xml\n",
      "en-zh/IWSLT15.TED.tst2012.en-zh.zh.xml\n",
      "en-zh/IWSLT15.TED.tst2013.en-zh.en.xml\n",
      "en-zh/IWSLT15.TED.tst2013.en-zh.zh.xml\n",
      "en-zh/README\n",
      "en-zh/train.tags.en-zh.en\n",
      "en-zh/train.tags.en-zh.zh\n",
      "en-zh/train.zh\n"
     ]
    }
   ],
   "source": [
    "# ! tar zxvf en-zh.tgz -C ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Prepare the word 2 vector model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parent_path(curPath):\n",
    "    parPath = curPath[:curPath.rfind(os.sep)]\n",
    "    return parPath\n",
    "\n",
    "class W2V_model_wrapper():\n",
    "    def __init__(self, w2vModel = None, w2vModelPath = None, vocabulary = None, name = ''):\n",
    "        self.name = name\n",
    "        self.w2vModelPath = w2vModelPath\n",
    "        self.w2vModel = w2vModel or KeyedVectors.load_word2vec_format(w2vModelPath)\n",
    "        self.vocabulary = vocabulary or ['<pad>', '<sos>', '<eos>', '<unk>']   # this order is important,\n",
    "        self.idx2Language = {}\n",
    "        \n",
    "    def _generate_idx2Language(self):\n",
    "        self.vocabulary.extend(self.w2vModel.index2word)\n",
    "        for idx in range(len(self.vocabulary)):\n",
    "            term = self.vocabulary[idx]\n",
    "            self.idx2Language[idx] = term\n",
    "    \n",
    "    def persist_idx2Language(self):\n",
    "        persistDir = get_parent_path(self.w2vModelPath)\n",
    "        persistPath = os.path.join(persistDir, self.name + '_idx2Lang')\n",
    "    \n",
    "        self._generate_idx2Language()\n",
    "        with open(persistPath, 'w', encoding = 'utf-8') as f:\n",
    "            for idx in self.idx2Language:\n",
    "                line = '%d\\t%s\\n'%(idx, self.idx2Language[idx])\n",
    "                f.write(line)\n",
    "        \n",
    "        print('idx2Lang persisted')\n",
    "        print(persistPath)\n",
    "    \n",
    "    def persist_weights(self):\n",
    "        persistDir = get_parent_path(self.w2vModelPath)\n",
    "        persistPath = os.path.join(persistDir, self.name + '_cleaned_weights')\n",
    "        \n",
    "        np.savetxt(persistPath, self.w2vModel.syn0, fmt = '%g')\n",
    "        print('cleaned_weights persisted')\n",
    "        print(persistPath)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enW2v = W2V_model_wrapper(w2vModel = w2vModel, w2vModelPath = enW2VPath, vocabulary = None, name = 'enW2v')\n",
    "# chW2v = W2V_model_wrapper(w2vModel = chW2vModel, w2vModelPath = chW2VPath, vocabulary = None, name = 'chW2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx2Lang persisted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:35: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_weights persisted\n"
     ]
    }
   ],
   "source": [
    "# enW2v.persist_idx2Language()\n",
    "# enW2v.persist_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx2Lang persisted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:35: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_weights persisted\n"
     ]
    }
   ],
   "source": [
    "# chW2v.persist_idx2Language()\n",
    "# chW2v.persist_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pre-processing  \n",
    "use **tf.dataset**, functionalities: \n",
    "1. clean illegal characters, add space\n",
    "2. tokenize\n",
    "3. add **sos** **eos** tags\n",
    "4. skip line according to pattern\n",
    "4. indexing  \n",
    "6. persist maps, processed data   \n",
    "5. map sentence to tensor  \n",
    "7. persist training data (indices)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1. Clean and add space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_en(text):\n",
    "    textTemp = re.sub(r'(?<=\\d),\\s*(?=\\d)', '', text)    # eliminate the digits seperator\n",
    "    textTemp = re.sub(r'((?<![\\d\\s])\\s*-)|(-\\s*(?!\\d))', ' ', textTemp)    # substitue the '-' symbol as space, except minus mark\n",
    "    \n",
    "    textTemp = re.sub(r'([?!,.])+', r'\\1', textTemp)    # eliminate the duplicate punctuations, not good enough?\n",
    "    textTemp = re.sub(r'([?!,. \\+\\-\\*/\\=\\%\\& \\'])', r' \\1 ', textTemp)    # ref: link 2, add spaces before and after [?!,.] marks\n",
    "    textTemp = re.sub(r'[^ a-z0-9A-Z?!,. \\+\\-\\*/\\=\\%\\&]', \"\", textTemp)\n",
    "    textTemp = re.sub(r'\\s+', ' ', textTemp)\n",
    "    textCleaned = textTemp.lower().strip()    #  eliminate space on the last position of sentence\n",
    "    return textCleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_ch(text):\n",
    "    textTemp = re.sub(r'(?<=\\d)，\\s*(?=\\d)', '', text)    # eliminate the digits seperator\n",
    "    textTemp = re.sub(r'((?<![\\d\\s])\\s*-)|(-\\s*(?!\\d))', ' ', textTemp)    # substitue the non-minus '-' symbol as space\n",
    "    \n",
    "    textTemp = re.sub(r'([？！，。、])+', r'\\1', textTemp)    # remove duplication\n",
    "    textTemp = re.sub(r'([？！，。、 \\+\\-\\*/\\=\\%\\&])', r' \\1 ', textTemp)    # add space\n",
    "    textTemp = re.sub(r'[^ \\u4e00-\\u9fa5a-z0-9A-Z？！，。、 \\+\\-\\*/\\=\\%\\&]', \"\", textTemp)    # ref: link 4\n",
    "    textTemp = re.sub(r'\\s+', '', textTemp)    # the space will be treated as a term when using jieba tokenizer, so eliminated here\n",
    "    textCleaned = textTemp.lower().strip()    #eliminate in the tokenize\n",
    "    return textCleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2. Tokenize  \n",
    "*tokenize firstly is for preventing the added tags being damaged by the tokenizer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_en(text):\n",
    "    termList = text.split(r' ')\n",
    "    return termList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_ch(text):\n",
    "    termList = jieba.cut(text)\n",
    "    return list(termList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3. Add_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tag(termList):\n",
    "    '''\n",
    "    directly operate the original term list, for saving the memeory\n",
    "    '''\n",
    "    termList.insert(0, '<sos>')\n",
    "    termList.append('<eos>')\n",
    "    return termList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.4 Skip line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_line(line):\n",
    "    skipSignal = True\n",
    "    skipPattern = '(^<[^\\n]*>)|(^\\s*$)'    # here may waste some useful training data, like the title\n",
    "    if len(re.findall(skipPattern, line)) == 0:    # not existing the pattern\n",
    "        skipSignal = False\n",
    "    return skipSignal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.5, 6, 7, 8. Index; Map from sentence to tensor; Persist maps, processed data; Persist training data (indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor():\n",
    "    '''\n",
    "    creat for each languages.\n",
    "    process the sentence, and map terms into indcies\n",
    "    while the language map is created internally, can also be imported from outside\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, clean = None, tokenize = None, add_tag = None, skip_line = None, vocabularySize = None):\n",
    "        self.clean = clean\n",
    "        self.tokenize = tokenize\n",
    "        self.add_tag = add_tag\n",
    "        self.skip_line = skip_line\n",
    "        self.vocabularySize = vocabularySize    # only use topK terms to construct the maps | deprecated when load the language model\n",
    "        \n",
    "        self.language2Idx = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>':3}    # <sos> <eos> is for distinguishing the same term in the first and last position \n",
    "        self.idx2Language = {0: '<pad>', 1: '<sos>', 2: '<eos>', 3: '<unk>'}\n",
    "        \n",
    "        self.languageDatasetPath = None\n",
    "        self.idx2LanguagePath = None    # only persist one map, as duplication\n",
    "        \n",
    "    \n",
    "    def clean_tokenize_addTag(self, sentence):\n",
    "        '''\n",
    "        1. eliminate illegal characters\n",
    "        2. cut up terms\n",
    "        3. add tags\n",
    "        '''\n",
    "        sentenceCleaned = self.clean(sentence)\n",
    "        termList = self.tokenize(sentenceCleaned)\n",
    "        termList = add_tag(termList)\n",
    "        return termList\n",
    "    \n",
    "    \n",
    "    def get_skipped_line_set(self, languageDatasetPath):\n",
    "        '''\n",
    "        generate the skip line set, union with the set of target language\n",
    "        '''\n",
    "        lineCnt = 0\n",
    "        skippedLineSet = set()\n",
    "        self.languageDatasetPath = languageDatasetPath\n",
    "        \n",
    "        with open(self.languageDatasetPath, 'r', encoding = 'utf-8') as langData:\n",
    "            \n",
    "            for line in langData:\n",
    "                lineCnt += 1\n",
    "                if self.skip_line(line):\n",
    "                    skippedLineSet.add(lineCnt)\n",
    "\n",
    "        return skippedLineSet\n",
    "    \n",
    "    \n",
    "    def load_language(self, languageDatasetPath = None, persistProcessed = False, skippedLineSet = None):\n",
    "        '''\n",
    "        load the language to genereate language map, and\n",
    "        persistProcessed, whether to persist the intermediate processed dataset\n",
    "        '''\n",
    "        self.languageDatasetPath = self.languageDatasetPath or languageDatasetPath    # set the self.languageDatasetPath only when it was not set\n",
    "        languageDatasetProcessedPath = self.languageDatasetPath + '_processed'\n",
    "        \n",
    "        # map paths\n",
    "        self.idx2LanguagePath = self.languageDatasetPath + '_idx2Lang'\n",
    "        \n",
    "        # use the counter to get topK terms for form the maps\n",
    "        languageCounter = Counter()\n",
    "        \n",
    "        # open the target processed dataset file firstly\n",
    "        if persistProcessed:\n",
    "            langDataProcessed = open(languageDatasetProcessedPath, 'w', encoding = 'utf-8')\n",
    "            \n",
    "        try:\n",
    "            with open(self.languageDatasetPath, 'r', encoding = 'utf-8') as langData:\n",
    "                lineCnt = 0\n",
    "                skippedLineSet = skippedLineSet or set()\n",
    "                \n",
    "                for line in langData:\n",
    "\n",
    "                    # show the progress\n",
    "                    lineCnt += 1\n",
    "                    if lineCnt % 30000 == 0:\n",
    "                        # break    # TEST\n",
    "                        print(lineCnt)\n",
    "\n",
    "                    if not lineCnt in skippedLineSet:\n",
    "                        termList = self.clean_tokenize_addTag(line.strip())\n",
    "\n",
    "                        # fill the language maps\n",
    "                        for term in termList: \n",
    "                            languageCounter.update([term])    # update with the term, instead of the characters\n",
    "\n",
    "                            # TODO: check what leads to the empty term\n",
    "                            if term == '':\n",
    "                                print(line)\n",
    "\n",
    "\n",
    "                        # if persist the intermediate procedded data\n",
    "                        if persistProcessed:\n",
    "                            # TODO: add log\n",
    "                            sentenceProcessed = ' '.join(termList) + '\\n'\n",
    "                            langDataProcessed.write(sentenceProcessed)\n",
    "                \n",
    "            # add the most common terms into map\n",
    "            languageCounter.pop('<sos>')\n",
    "            languageCounter.pop('<eos>')\n",
    "            for term, freq in languageCounter.most_common(self.vocabularySize):             \n",
    "                idx = len(self.language2Idx)    # as count from 0\n",
    "                self.language2Idx[term] = idx\n",
    "                self.idx2Language[idx] = term\n",
    "                                \n",
    "                                \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "        finally:\n",
    "            if persistProcessed:    # so that the langDataProcessed will not be used before define\n",
    "                langDataProcessed.close()\n",
    "            \n",
    "            \n",
    "    def persist_languange_map(self):\n",
    "        '''\n",
    "        save the generated language map\n",
    "        '''\n",
    "        if type(self.idx2LanguagePath) != type(None):\n",
    "            with open(self.idx2LanguagePath, 'w', encoding = 'utf-8') as f:\n",
    "                for idx in self.idx2Language:\n",
    "                    line = '%d\\t%s\\n'%(idx, self.idx2Language[idx])\n",
    "                    f.write(line)\n",
    "        else:\n",
    "            print('the idx2Language map is not assigned yet, i.e. the idx2Language map is not existing yet')\n",
    "            \n",
    "        # TODO: add log\n",
    "            \n",
    "                            \n",
    "        \n",
    "    def load_language_maps(self, idx2LanguagePath):\n",
    "        '''\n",
    "        use an existing map\n",
    "        '''\n",
    "        self.idx2LanguagePath = idx2LanguagePath\n",
    "        \n",
    "        with open(self.idx2LanguagePath, 'r', encoding = 'utf-8') as f:\n",
    "            for line in f:\n",
    "                idxTermPair = line.strip().split('\\t')\n",
    "                if len(idxTermPair) > 1:\n",
    "                    idx = idxTermPair[0]\n",
    "                    term = idxTermPair[1]\n",
    "                    self.language2Idx[term] = int(idx)    # NOTE: aware the index type is int\n",
    "                    self.idx2Language[int(idx)] = term\n",
    "                else:\n",
    "                    print(idxTermPair)    # if there exists empty term\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    methods belog are making use of the generated/loaded map\n",
    "    '''\n",
    "    def index_term(self, term):\n",
    "        '''\n",
    "        transform term into index\n",
    "        '''\n",
    "        return self.language2Idx.get(term) or 3    # if the term is not seen before, return the index of <unk> tag\n",
    "    \n",
    "    \n",
    "    def index_raw_sentence(self, rawSentence):\n",
    "        '''\n",
    "        transform sentence into indices\n",
    "        - require methods: clean, tokenize, add_tag, skip_line\n",
    "        - for tf.dataset to apply on raw sentence retrive the tensor\n",
    "        '''\n",
    "        termList = self.clean_tokenize_addTag(rawSentence)\n",
    "        termIdxList = list(map(self.index_term, termList))\n",
    "        return termIdxList\n",
    "    \n",
    "    \n",
    "    def index_persist_processed_data(self, languageDatasetProcessedPath):\n",
    "        '''\n",
    "        index the processed data set and persist as the training dataset (indices)\n",
    "        not necessarily the language data set that used to create the map\n",
    "        \n",
    "        when processing the raw data set, use tf.dataset api to apply index_sentence on the raw sentences\n",
    "        '''\n",
    "        senLenList = []\n",
    "        with open(languageDatasetProcessedPath, 'r', encoding = 'utf-8') as f:\n",
    "            with open(languageDatasetProcessedPath + '_index', 'w', encoding = 'utf-8') as h:\n",
    "                for line in f:\n",
    "                    termList = line.strip().split(' ')\n",
    "                    idxList = list(map(str, map(self.index_term, termList)))    # transform index into int format\n",
    "                    h.write(' '.join(idxList) + '\\n')\n",
    "        \n",
    "                    senLenList.append(len(termList))\n",
    "        \n",
    "        maxSenLen = max(senLenList)\n",
    "        print('tensor max length: %d'%maxSenLen)\n",
    "        return senLenList\n",
    "        # TODO: add log\n",
    "        \n",
    "        \n",
    "    def retrive_terms(self, idxList):\n",
    "        termList = []\n",
    "        for idx in idxList:\n",
    "            termList.append(self.idx2Language[idx])\n",
    "        return ' '.join(termList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.9. Run preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # load the original language data set into preprocessor to generate the language maps and intermediate processed (tokenized) data\n",
    "# pEn = Preprocessor(clean_en, tokenize_en, add_tag, skip_line, enVocabularySize)\n",
    "# pCh = Preprocessor(clean_ch, tokenize_ch, add_tag, skip_line, chVocabularySize)\n",
    "\n",
    "# enSkippedLineSet = pEn.get_skipped_line_set(enPath)\n",
    "# chSkippedLineSet = pCh.get_skipped_line_set(chPath)\n",
    "\n",
    "# skippedLineSet = enSkippedLineSet.union(chSkippedLineSet)    # for align the source dataset and target dataset\n",
    "\n",
    "# pEn.load_language(persistProcessed = True, skippedLineSet = skippedLineSet)\n",
    "# pCh.load_language(persistProcessed = True, skippedLineSet = skippedLineSet)\n",
    "\n",
    "# pEn.persist_languange_map()\n",
    "# pCh.persist_languange_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pEn.index_raw_sentence('This is a good day dssd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_bak\t\t       glove.6B.50d_gensim.txt\t   sgns.baidubaike.bigram-char\r\n",
      "enW2v_cleaned_weights  glove.6B.50d_gensim.txt.gz\r\n",
      "enW2v_idx2Lang\t       original\r\n"
     ]
    }
   ],
   "source": [
    "!ls {datasetPath}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400004\n",
      "4004\n",
      "tensor max length: 616\n",
      "tensor max length: 600\n",
      "CPU times: user 11.7 s, sys: 3.63 s, total: 15.3 s\n",
      "Wall time: 16.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load the language maps and generate the index of language datasets\n",
    "pEn2 = Preprocessor()\n",
    "pCh2 = Preprocessor()\n",
    "\n",
    "pEn2.load_language_maps(os.path.join(datasetPath, 'enW2v_idx2Lang'))    # here make use of the maps from trained embeddings\n",
    "# pCh2.load_language_maps(os.path.join(datasetPath, 'chW2v_idx2Lang'))\n",
    "pCh2.load_language_maps(chPath + '_idx2Lang')    # use the original Chinese model\n",
    "\n",
    "print(len(pEn2.idx2Language))\n",
    "print(len(pCh2.idx2Language))\n",
    "\n",
    "enSenLenList = pEn2.index_persist_processed_data(enPath + '_processed')\n",
    "chSenLenList = pCh2.index_persist_processed_data(chPath + '_processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    49.0\n",
      "Name: 0.95, dtype: float64\n",
      "0    43.0\n",
      "Name: 0.95, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# # set the padding length as 50, so that over 95% sentences are coverd\n",
    "# enSenDF = pd.DataFrame(enSenLenList)\n",
    "# chSenDF = pd.DataFrame(chSenLenList)\n",
    "# print(enSenDF.quantile(0.95))\n",
    "# print(chSenDF.quantile(0.95))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split the training / testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "enL = []\n",
    "with open(enPath + '_processed_index', 'r', encoding = 'utf-8') as enF:\n",
    "    for line in enF:\n",
    "        enL.append(line)\n",
    "        \n",
    "chL = []\n",
    "with open(chPath + '_processed_index', 'r', encoding = 'utf-8') as chF:\n",
    "    for line in chF:\n",
    "        chL.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "enTrain, enTest, chTrain, chTest = train_test_split(enL, chL, test_size=0.01, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(enPath + '_processed_index_train', 'w', encoding = 'utf-8') as enF:\n",
    "    for line in enTrain:\n",
    "        enF.write(line)\n",
    "        \n",
    "with open(enPath + '_processed_index_test', 'w', encoding = 'utf-8') as enF:\n",
    "    for line in enTest:\n",
    "        enF.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(chPath + '_processed_index_train', 'w', encoding = 'utf-8') as chF:\n",
    "    for line in chTrain:\n",
    "        chF.write(line)\n",
    "        \n",
    "with open(chPath + '_processed_index_test', 'w', encoding = 'utf-8') as chF:\n",
    "    for line in chTest:\n",
    "        chF.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(indexFilePath, maxSenLen = 50): # ref: 1\n",
    "    '''\n",
    "    Shared by both English and Chinese dataset, return the defined tf.dataset object\n",
    "    '''\n",
    "    dataset = tf.data.TextLineDataset(indexFilePath)\n",
    "    dataset = dataset.map( \n",
    "                            lambda line:  tf.strings.split(tf.strings.strip([line]), sep = ' ').values\n",
    "                         )\n",
    "    dataset = dataset.map(\n",
    "                            lambda strList: tf.strings.to_number(strList, tf.int32) \n",
    "                         )\n",
    "    \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_input(xIdxFilePath, yIdxFilePath, maxSenLen = 50, batchSize = 250, vocabSize = 4000):    # , vocabularySize = 1000\n",
    "    # ref: 1\n",
    "    # bufferSize = 1000, prefetchSize = 10, seed = 1\n",
    "    \n",
    "    xDataSet = get_dataset(xIdxFilePath, maxSenLen)\n",
    "    yDataSet = get_dataset(yIdxFilePath, maxSenLen)\n",
    "    dataset = tf.data.Dataset.zip((xDataSet, yDataSet))\n",
    "    \n",
    "    def filter_length(xIdxList, yIdxList):\n",
    "        '''\n",
    "        <= maxSenLen + 1, as <eos> / <sos> will be removed\n",
    "        when pad back, maxLen again\n",
    "        '''\n",
    "        xSenLen = tf.size(xIdxList)\n",
    "        ySenLen = tf.size(yIdxList)\n",
    "        xSignal = tf.logical_and(tf.greater(xSenLen, 2), tf.less_equal(xSenLen, maxSenLen))\n",
    "        ySignal = tf.logical_and(tf.greater(ySenLen, 2), tf.less_equal(ySenLen, maxSenLen))    \n",
    "        recordSignal = tf.logical_and(xSignal, ySignal)\n",
    "        return recordSignal\n",
    "    \n",
    "    dataset = dataset.filter(filter_length)    # not cut short, directly get rid of the too long sentence\n",
    "    dataset = dataset.shuffle(batchSize * 10)    # TODO extract as parameter, re-ini will re-shuffle the dataset, so does not use the seed here, for simplicity\n",
    "    \n",
    "#     def add_label_modify_tag(xIdxList, yIdxList):\n",
    "#         '''\n",
    "#         add the label column, \n",
    "#         and modify the <sos> and <eos> tag of input and label\n",
    "#         '''\n",
    "#         inputY = tf.one_hot(yIdxList, depth = vocabSize)\n",
    "#         label = tf.one_hot(yIdxList[1:], depth = vocabSize)\n",
    "        \n",
    "#         return (xIdxList, inputY), label    # xIdxList <sos..eos>, yIdxList <sos..eos>, labelList <..eos>\n",
    "    \n",
    "#     dataset = dataset.map(lambda x,y: add_label_modify_tag(x, y))\n",
    "    \n",
    "#     # padding shape of each pair of sentences (bilingal), does not necessarily have identical shape[1] among multiple batches?\n",
    "#     dataset = dataset.padded_batch(batchSize, padded_shapes = (((maxSenLen, ), (maxSenLen,None)), (maxSenLen,None)), drop_remainder = True)\n",
    "\n",
    "    def add_label_modify_tag(xIdxList, yIdxList):\n",
    "        '''\n",
    "        add the label column, \n",
    "        and modify the <sos> and <eos> tag of input and label\n",
    "        '''\n",
    "        yOHList = tf.one_hot(yIdxList, depth = vocabSize + 4)\n",
    "        return xIdxList, yOHList\n",
    "    \n",
    "    dataset = dataset.map(lambda x,y: add_label_modify_tag(x, y))\n",
    "    \n",
    "    # padding shape of each pair of sentences (bilingal), does not necessarily have identical shape[1] among multiple batches?\n",
    "    dataset = dataset.padded_batch(batchSize, padded_shapes = ((maxSenLen, ), (maxSenLen, None)), drop_remainder = True)\n",
    "    \n",
    "#     dataset = dataset.prefetch(batchSize * 10)\n",
    "    \n",
    "    dataset = dataset.repeat()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 20) (100, 20, 4004)\n",
      "<class 'tuple'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "[[[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n",
      "(100, 20) (100, 20, 4004)\n",
      "<class 'tuple'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "[[[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n",
      "(100, 20) (100, 20, 4004)\n",
      "<class 'tuple'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "[[[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n",
      "(100, 20) (100, 20, 4004)\n",
      "<class 'tuple'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "[[[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 1. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n",
      "(100, 20) (100, 20, 4004)\n",
      "<class 'tuple'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "[[[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 1. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n",
      "(100, 20) (100, 20, 4004)\n",
      "<class 'tuple'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "[[[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n",
      "\n",
      "CPU times: user 9.8 s, sys: 453 ms, total: 10.3 s\n",
      "Wall time: 16.6 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# dataset = get_model_input(enPath + '_processed_index', chPath + '_processed_index', \n",
    "#                             maxSenLen = maxSenLen, batchSize = batchSize, vocabSize = chVocabularySize) \n",
    "#                                 # bufferSize = bufferSize, prefetchSize = prefetchSize, seed = seed, vocabularySize = chVocabularySize\n",
    "\n",
    "# iterator = dataset.make_initializable_iterator()\n",
    "# next = iterator.get_next()\n",
    "# ini = iterator.initializer\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     for i in range(1):\n",
    "#         sess.run(ini)    # to see if the random status is reset each time the iterator is re-initialized\n",
    "#         for i in range(6):\n",
    "#             pair = sess.run(next)\n",
    "# #             x, y = pair[0]\n",
    "# #             label = pair[1]\n",
    "# #             print(x.shape, y.shape, label.shape)\n",
    "# #             print(type(pair), type(x), type(y), type(label))\n",
    "# #             print(x, y, label)\n",
    "\n",
    "#             x = pair[0]\n",
    "#             y = pair[1]\n",
    "# #             label = pair[2]\n",
    "#             print(x.shape, y.shape) # , label.shape\n",
    "#             print(type(pair), type(x), type(y)) # , type(label)\n",
    "#             print(y)\n",
    "\n",
    "#         print()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Attention model 2, v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![architecture](./architecture.png)  \n",
    "**--> Fig comes from ref 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator2():\n",
    "    def __init__(self, \n",
    "                 logger = None, \n",
    "                 loadTrainedModel = False,     # load trained translator\n",
    "                 \n",
    "                 modelName = None,    # persistance information\n",
    "                 version = None,\n",
    "                 checkpointDir = None, \n",
    "                 \n",
    "                 loadSrcLanguageModel = False,    # load source trained language model, e.g. glov\n",
    "                 srcLanguageModelPath = None,\n",
    "                 loadTarLanguageModel = False,    # load target trained language model, e.g. \n",
    "                 tarLanguageModelPath = None,\n",
    "\n",
    "                 senMaxLen = None,\n",
    "                 hiddenUnitNum = None,    # LSTM units\n",
    "                 srcVocabularySize = None,    # source language embedding input size\n",
    "                 tarVocabularySize = None,    # target language embedding input size\n",
    "                 srcEmbeddingSize = None,    # embedding output size\n",
    "                 tarEmbeddingSize = None,  \n",
    "                 energySize = None,    # energy size of attention mechanism\n",
    "                 \n",
    "                 droupOutRatio = None, \n",
    "                 batchSize = None, \n",
    "                 epochNum = None, \n",
    "                 verbose = None, \n",
    "                 validationRatio = None, \n",
    "                 patience = None):    # seperate the data feeding procedure out of the model\n",
    "\n",
    "        \n",
    "        self.lg = logger or Logger.get_logger(modelName)\n",
    "        self.loadTrainedModel = loadTrainedModel    # use the trained classifier or not\n",
    "        \n",
    "        self.modelName = modelName    # persistance information\n",
    "        self.version = version\n",
    "        self.checkpointDir = checkpointDir\n",
    "        self.checkpointPath = os.path.join(checkpointDir, modelName, str(version), 'train.ckpt')\n",
    "        self.checkpointVisualPath = os.path.join(checkpointDir, modelName, str(version), 'visualization')\n",
    "\n",
    "        self.loadSrcLanguageModel = loadSrcLanguageModel    # load source trained language model, e.g. glov\n",
    "        self.srcLanguageModelPath = srcLanguageModelPath\n",
    "        self.loadTarLanguageModel = loadTarLanguageModel    # load target trained language model, e.g. \n",
    "        self.tarLanguageModelPath = tarLanguageModelPath\n",
    "\n",
    "        self.senMaxLen = senMaxLen    # input shape of the Input layer\n",
    "        self.hiddenUnitNum = hiddenUnitNum    # LSTM units\n",
    "        self.srcVocabularySize = srcVocabularySize\n",
    "        self.tarVocabularySize = tarVocabularySize\n",
    "        self.srcEmbeddingSize = srcEmbeddingSize    # embedding output size\n",
    "        self.tarEmbeddingSize = tarEmbeddingSize\n",
    "        self.energySize = energySize\n",
    "        \n",
    "        '''set the sizes according to the loaded language model'''\n",
    "        if loadSrcLanguageModel:\n",
    "            self.srcW2VModel = KeyedVectors.load_word2vec_format(srcLanguageModelPath)\n",
    "            self.srcVocabularySize = len(self.srcW2VModel.index2word)\n",
    "            self.srcEmbeddingSize = self.srcW2VModel.vector_size\n",
    "        if loadTarLanguageModel:\n",
    "            self.tarW2VModel = KeyedVectors.load_word2vec_format(tarLanguageModelPath)\n",
    "            self.tarVocabularySize = len(self.tarW2VModel.index2word)\n",
    "            self.tarEmbeddingSize = self.tarW2VModel.vector_size\n",
    "        \n",
    "        # Hyper parameters are set for the convenience of reload the trained model for continue training\n",
    "        self.droupOutRatio = droupOutRatio \n",
    "        self.batchSize = batchSize\n",
    "        self.epochNum = epochNum\n",
    "        self.verbose = verbose\n",
    "        self.validationRatio = validationRatio\n",
    "        self.patience = patience\n",
    "        \n",
    "        self.model = None\n",
    "        self.predictModel = None\n",
    "        self._ini_model() \n",
    "    \n",
    "\n",
    "    def _load_model(self):\n",
    "        loadedFlag = False\n",
    "        try:\n",
    "            self.model.load_weights(self.checkpointPath)\n",
    "            loadedFlag = True\n",
    "            self.lg.info('---> pre-trained translator loaded')\n",
    "        except Exception as e:\n",
    "            traceback.format_exc(e)\n",
    "            \n",
    "        return loadedFlag\n",
    "\n",
    "    \n",
    "    def _load_language_model(self, embeddingName, w2VModel):\n",
    "        embeddingWeights = np.concatenate([np.zeros((4, w2VModel.vector_size)), w2VModel.syn0], axis = 0)    # add vectors for helping tags <pad> <sos> <eos> <unk>\n",
    "        self.model.get_layer(embeddingName).set_weights([embeddingWeights])    # ref: 22\n",
    "        self.lg.info('%s embedding is loaded'%embeddingName)\n",
    "    \n",
    "    \n",
    "    def _ini_model(self):\n",
    "        self._construct_model()\n",
    "        \n",
    "        if self.loadTrainedModel:\n",
    "            '''\n",
    "            load pre-trained model\n",
    "            '''\n",
    "            loadedFlag = self._load_model()\n",
    "            \n",
    "        if self.loadSrcLanguageModel:\n",
    "            '''\n",
    "            load trained w2v model into embedding layer\n",
    "            '''\n",
    "            self._load_language_model('srcLanguageModel', self.srcW2VModel)\n",
    "            \n",
    "        if self.loadTarLanguageModel:\n",
    "            self._load_language_model('tarLanguageModel', self.tarW2VModel)\n",
    "            \n",
    "            \n",
    "    def __LSTM_creator(self, hiddenUnitNum, return_sequences = True, return_state = True):    # ref: 2\n",
    "        layer = None\n",
    "        if tf.test.is_gpu_available():\n",
    "            layer = tf.keras.layers.CuDNNLSTM(hiddenUnitNum, return_sequences = return_sequences, return_state = return_state)\n",
    "        else:\n",
    "            layer = tf.keras.layers.LSTM(hiddenUnitNum, return_sequences = return_sequences, return_state = return_state)\n",
    "        return layer\n",
    "    \n",
    "    \n",
    "    def _construct_model(self):\n",
    "            \n",
    "        # layers\n",
    "        srcSenIdx = tf.keras.layers.Input((self.senMaxLen, ), name = 'sourceLanguageIdx')    # the <eos> is cut\n",
    "#         tarSenIdx = tf.keras.layers.Input((self.senMaxLen, ), name = 'tarLanguageIdx')    # the <eos> is cut\n",
    "\n",
    "        hiddenUnitNum = self.hiddenUnitNum    # ref: 12\n",
    "        statusIniLayer = tf.keras.layers.Lambda(lambda x:tf.zeros((tf.shape(x)[0], hiddenUnitNum)), \n",
    "                                                name = 'statusIniLayer')    # dynamic size according to batch size, ref: 10\n",
    "\n",
    "#         srcLanguageModel = tf.keras.layers.Embedding(self.srcVocabularySize + 4, self.srcEmbeddingSize, name = 'srcLanguageModel') \n",
    "#         tarLanguageModel = tf.keras.layers.Embedding(self.tarVocabularySize + 4, self.tarEmbeddingSize, name = 'tarLanguageModel')\n",
    "\n",
    "        srcLanguageModel = tf.keras.layers.Embedding(self.srcVocabularySize + 4, self.srcEmbeddingSize, name = 'srcLanguageModel', trainable = False) \n",
    "            # [batchSize, senMaxLen, embeddingSize]\n",
    "\n",
    "        preAtLSTM = tf.keras.layers.Bidirectional(self.__LSTM_creator(self.hiddenUnitNum, return_state = False))    # a shape: (hiddenStateNum * 2)\n",
    "        postAtLSTM = self.__LSTM_creator(self.hiddenUnitNum, return_sequences = False)    # c = tanh(w[a, x]) w ensures c has the correct length == len(a)\n",
    "\n",
    "        concateTarContext = tf.keras.layers.Concatenate()    # for concate the target sentence and context together\n",
    "\n",
    "        outputLayer = tf.keras.layers.Dense(self.tarVocabularySize + 4, activation = 'linear', name = 'outputLayer')    # each step output 1 index\n",
    "\n",
    "        flatten = tf.keras.layers.Flatten()\n",
    "\n",
    "        expandDimLayer = tf.keras.layers.Lambda(lambda x:tf.expand_dims(x, 1), name = 'expandDimLayer')\n",
    "\n",
    "        getSosIdx = tf.keras.layers.Lambda(lambda x: tf.ones(tf.shape(x)[0], ), name = 'getSosIdx')\n",
    "            # get first step <sos> indices with shape(None, )\n",
    "\n",
    "        distribution2IndexLayer = tf.keras.layers.Lambda(lambda x: tf.math.argmax(x, axis = 1),  # tf.expand_dims(\n",
    "                                                         name = 'distribution2IndexLayer')    \n",
    "            # translate the ouput distribition over terms into index on terms\n",
    "            # also maintain the last dim which only contains 1 element\n",
    "\n",
    "        convertSwapLayer2D = tf.keras.layers.Lambda(lambda xList : tf.transpose(tf.convert_to_tensor(xList), perm = [1, 0]), name = 'convertSwapLayer2D')\n",
    "        convertSwapLayer3D = tf.keras.layers.Lambda(lambda xList : tf.transpose(tf.convert_to_tensor(xList), perm = [1, 0, 2]), name = 'convertSwapLayer3D')\n",
    "\n",
    "        dropoutLayer = tf.keras.layers.Dropout(0.2)\n",
    "\n",
    "        # layers for attention\n",
    "        repeatLayer = tf.keras.layers.RepeatVector(self.senMaxLen)\n",
    "        concatePrePostStates = tf.keras.layers.Concatenate(axis = -1)\n",
    "        dense1 = tf.keras.layers.Dense(self.energySize, activation = 'tanh')\n",
    "        dense2 = tf.keras.layers.Dense(1, activation = 'relu')\n",
    "        softmaxLayer = tf.keras.layers.Softmax(axis = -1)\n",
    "        dotLayer = tf.keras.layers.Dot(axes = 1)\n",
    "\n",
    "\n",
    "        # apply layer\n",
    "        '''attention'''\n",
    "        def __attention(preAtHiddenStateSeq, postAtHiddenState):    # ref: 3, 2\n",
    "            '''\n",
    "            attentions, the weights of activations of encoder\n",
    "\n",
    "            preAtHiddenStateSeq with shape (m, senLen, 2 * n_a), 2 * n_a means 'bi-directional'\n",
    "                the pre attention RNN ran with forward and backward before the post attention RNN ran.\n",
    "\n",
    "            energy is the output of w([s, a_bi]) + b, input of softmax to get the attention\n",
    "            '''\n",
    "\n",
    "            postStates = repeatLayer(postAtHiddenState)\n",
    "            concateTensors = concatePrePostStates([preAtHiddenStateSeq, postStates])\n",
    "            temp = dense1(concateTensors)\n",
    "            energies = dense2(temp)    # not scaled attentions\n",
    "            attentions = softmaxLayer(energies)\n",
    "            context = dotLayer([attentions, preAtHiddenStateSeq])    # three is a broadcasting with attentions in this procedure\n",
    "\n",
    "            return context\n",
    "\n",
    "\n",
    "        '''training model'''\n",
    "        srcSenEmb = dropoutLayer(srcLanguageModel(srcSenIdx))    # ref: 13\n",
    "#         tarSenEmb = dropoutLayer(tarLanguageModel(tarSenIdx))\n",
    "\n",
    "        preAtHiddenStateSeq = preAtLSTM(srcSenEmb)    # Bidirectional only return the merged output\n",
    "\n",
    "        outputSen = []\n",
    "\n",
    "        postAtHiddenState = statusIniLayer(srcSenIdx)\n",
    "        postAtCellState = statusIniLayer(srcSenIdx)\n",
    "\n",
    "        for step in range(self.senMaxLen):    # iterate over the target sentence, no <eos> tag\n",
    "            context = __attention(preAtHiddenStateSeq, postAtHiddenState)    # (hiddenStateNum*2, 1)\n",
    "            postAtHiddenState, _, postAtCellState = postAtLSTM(context, initial_state = [postAtHiddenState, postAtCellState])\n",
    "\n",
    "            outputDistribution = outputLayer(postAtHiddenState)    # word distribution\n",
    "            outputSen.append(outputDistribution)    # for calculate the loss\n",
    "\n",
    "        outputSenTranspose = convertSwapLayer3D(outputSen)    # (None, 20, vocab)\n",
    "\n",
    "        self.model = model = tf.keras.Model(inputs = srcSenIdx, outputs = outputSenTranspose, name = 'translator_v3')    # ref: 7, 11\n",
    "            # outputs convert to (batchSize, step, vocabulary)\n",
    "\n",
    "        '''loss'''\n",
    "        def __loss(real, pred):    # ref: 2\n",
    "            '''\n",
    "            make use of cross-entropy here\n",
    "            real: (batchSize, senMaxLen)\n",
    "            logit: (batchSize, senMaxLen, vocabularySize)\n",
    "            '''\n",
    "            mask = 1 - np.equal(real, 0)    # mask out the padded elements\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels = real, logits = pred) * mask)    # ref: 6, use unscaled logits\n",
    "            return loss\n",
    "\n",
    "        self.model.compile(optimizer = 'adam', loss = __loss)\n",
    "        self.model.summary()\n",
    "\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def train(self, train_input_fn = None, val_input_fn = None, epochs = 1, steps_per_epoch = 1, validation_steps = 1, verbose = 1):    # invoke the model.fit | , validation_split = 0.0\n",
    "        trainDataset = train_input_fn()    # ref: 9, with shape ((x, y), label)\n",
    "        valDataset = val_input_fn()\n",
    "        self.model.fit(trainDataset, validation_data = valDataset, \n",
    "                       epochs = epochs, steps_per_epoch = steps_per_epoch, \n",
    "                       verbose = verbose, validation_steps = validation_steps,\n",
    "                       callbacks = [tf.keras.callbacks.ModelCheckpoint(self.checkpointPath), \n",
    "                                    tf.keras.callbacks.TensorBoard(self.checkpointVisualPath, write_images = True)])    # , save_weights_only = 'True'\n",
    "                                                                                                 \n",
    "        \n",
    "    def evaluate(self, xIdxList, yOHList):\n",
    "        self.model.evaluate()\n",
    "        tf.keras.Model().evaluate([xIdxList], [yOHList])\n",
    "    \n",
    "    \n",
    "    def predict(self, srcSenIdx):    # invoke the model.evaluate\n",
    "        predSenIdx = self.model.predict([srcSenIdx,], batch_size = self.batchSize)\n",
    "        return predSenIdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sourceLanguageIdx (InputLayer)  (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "srcLanguageModel (Embedding)    (None, 20, 50)       20000200    sourceLanguageIdx[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 20, 50)       0           srcLanguageModel[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "statusIniLayer (Lambda)         (None, 64)           0           sourceLanguageIdx[0][0]          \n",
      "                                                                 sourceLanguageIdx[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 20, 128)      58880       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 20, 64)       0           statusIniLayer[0][0]             \n",
      "                                                                 lstm_3[0][0]                     \n",
      "                                                                 lstm_3[1][0]                     \n",
      "                                                                 lstm_3[2][0]                     \n",
      "                                                                 lstm_3[3][0]                     \n",
      "                                                                 lstm_3[4][0]                     \n",
      "                                                                 lstm_3[5][0]                     \n",
      "                                                                 lstm_3[6][0]                     \n",
      "                                                                 lstm_3[7][0]                     \n",
      "                                                                 lstm_3[8][0]                     \n",
      "                                                                 lstm_3[9][0]                     \n",
      "                                                                 lstm_3[10][0]                    \n",
      "                                                                 lstm_3[11][0]                    \n",
      "                                                                 lstm_3[12][0]                    \n",
      "                                                                 lstm_3[13][0]                    \n",
      "                                                                 lstm_3[14][0]                    \n",
      "                                                                 lstm_3[15][0]                    \n",
      "                                                                 lstm_3[16][0]                    \n",
      "                                                                 lstm_3[17][0]                    \n",
      "                                                                 lstm_3[18][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 20, 192)      0           bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[0][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[1][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[2][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[3][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[4][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[5][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[6][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[7][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[8][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[9][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[10][0]           \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[11][0]           \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[12][0]           \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[13][0]           \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[14][0]           \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[15][0]           \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[16][0]           \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[17][0]           \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[18][0]           \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[19][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 20, 10)       1930        concatenate_3[0][0]              \n",
      "                                                                 concatenate_3[1][0]              \n",
      "                                                                 concatenate_3[2][0]              \n",
      "                                                                 concatenate_3[3][0]              \n",
      "                                                                 concatenate_3[4][0]              \n",
      "                                                                 concatenate_3[5][0]              \n",
      "                                                                 concatenate_3[6][0]              \n",
      "                                                                 concatenate_3[7][0]              \n",
      "                                                                 concatenate_3[8][0]              \n",
      "                                                                 concatenate_3[9][0]              \n",
      "                                                                 concatenate_3[10][0]             \n",
      "                                                                 concatenate_3[11][0]             \n",
      "                                                                 concatenate_3[12][0]             \n",
      "                                                                 concatenate_3[13][0]             \n",
      "                                                                 concatenate_3[14][0]             \n",
      "                                                                 concatenate_3[15][0]             \n",
      "                                                                 concatenate_3[16][0]             \n",
      "                                                                 concatenate_3[17][0]             \n",
      "                                                                 concatenate_3[18][0]             \n",
      "                                                                 concatenate_3[19][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 20, 1)        11          dense_2[0][0]                    \n",
      "                                                                 dense_2[1][0]                    \n",
      "                                                                 dense_2[2][0]                    \n",
      "                                                                 dense_2[3][0]                    \n",
      "                                                                 dense_2[4][0]                    \n",
      "                                                                 dense_2[5][0]                    \n",
      "                                                                 dense_2[6][0]                    \n",
      "                                                                 dense_2[7][0]                    \n",
      "                                                                 dense_2[8][0]                    \n",
      "                                                                 dense_2[9][0]                    \n",
      "                                                                 dense_2[10][0]                   \n",
      "                                                                 dense_2[11][0]                   \n",
      "                                                                 dense_2[12][0]                   \n",
      "                                                                 dense_2[13][0]                   \n",
      "                                                                 dense_2[14][0]                   \n",
      "                                                                 dense_2[15][0]                   \n",
      "                                                                 dense_2[16][0]                   \n",
      "                                                                 dense_2[17][0]                   \n",
      "                                                                 dense_2[18][0]                   \n",
      "                                                                 dense_2[19][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "softmax_1 (Softmax)             (None, 20, 1)        0           dense_3[0][0]                    \n",
      "                                                                 dense_3[1][0]                    \n",
      "                                                                 dense_3[2][0]                    \n",
      "                                                                 dense_3[3][0]                    \n",
      "                                                                 dense_3[4][0]                    \n",
      "                                                                 dense_3[5][0]                    \n",
      "                                                                 dense_3[6][0]                    \n",
      "                                                                 dense_3[7][0]                    \n",
      "                                                                 dense_3[8][0]                    \n",
      "                                                                 dense_3[9][0]                    \n",
      "                                                                 dense_3[10][0]                   \n",
      "                                                                 dense_3[11][0]                   \n",
      "                                                                 dense_3[12][0]                   \n",
      "                                                                 dense_3[13][0]                   \n",
      "                                                                 dense_3[14][0]                   \n",
      "                                                                 dense_3[15][0]                   \n",
      "                                                                 dense_3[16][0]                   \n",
      "                                                                 dense_3[17][0]                   \n",
      "                                                                 dense_3[18][0]                   \n",
      "                                                                 dense_3[19][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1, 128)       0           softmax_1[0][0]                  \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 softmax_1[1][0]                  \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 softmax_1[2][0]                  \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 softmax_1[3][0]                  \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 softmax_1[4][0]                  \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 softmax_1[5][0]                  \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 softmax_1[6][0]                  \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 softmax_1[7][0]                  \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 softmax_1[8][0]                  \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 softmax_1[9][0]                  \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 softmax_1[10][0]                 \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 softmax_1[11][0]                 \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 softmax_1[12][0]                 \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 softmax_1[13][0]                 \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 softmax_1[14][0]                 \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 softmax_1[15][0]                 \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 softmax_1[16][0]                 \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 softmax_1[17][0]                 \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 softmax_1[18][0]                 \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 softmax_1[19][0]                 \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, 64), (None,  49408       dot_1[0][0]                      \n",
      "                                                                 statusIniLayer[0][0]             \n",
      "                                                                 statusIniLayer[1][0]             \n",
      "                                                                 dot_1[1][0]                      \n",
      "                                                                 lstm_3[0][0]                     \n",
      "                                                                 lstm_3[0][2]                     \n",
      "                                                                 dot_1[2][0]                      \n",
      "                                                                 lstm_3[1][0]                     \n",
      "                                                                 lstm_3[1][2]                     \n",
      "                                                                 dot_1[3][0]                      \n",
      "                                                                 lstm_3[2][0]                     \n",
      "                                                                 lstm_3[2][2]                     \n",
      "                                                                 dot_1[4][0]                      \n",
      "                                                                 lstm_3[3][0]                     \n",
      "                                                                 lstm_3[3][2]                     \n",
      "                                                                 dot_1[5][0]                      \n",
      "                                                                 lstm_3[4][0]                     \n",
      "                                                                 lstm_3[4][2]                     \n",
      "                                                                 dot_1[6][0]                      \n",
      "                                                                 lstm_3[5][0]                     \n",
      "                                                                 lstm_3[5][2]                     \n",
      "                                                                 dot_1[7][0]                      \n",
      "                                                                 lstm_3[6][0]                     \n",
      "                                                                 lstm_3[6][2]                     \n",
      "                                                                 dot_1[8][0]                      \n",
      "                                                                 lstm_3[7][0]                     \n",
      "                                                                 lstm_3[7][2]                     \n",
      "                                                                 dot_1[9][0]                      \n",
      "                                                                 lstm_3[8][0]                     \n",
      "                                                                 lstm_3[8][2]                     \n",
      "                                                                 dot_1[10][0]                     \n",
      "                                                                 lstm_3[9][0]                     \n",
      "                                                                 lstm_3[9][2]                     \n",
      "                                                                 dot_1[11][0]                     \n",
      "                                                                 lstm_3[10][0]                    \n",
      "                                                                 lstm_3[10][2]                    \n",
      "                                                                 dot_1[12][0]                     \n",
      "                                                                 lstm_3[11][0]                    \n",
      "                                                                 lstm_3[11][2]                    \n",
      "                                                                 dot_1[13][0]                     \n",
      "                                                                 lstm_3[12][0]                    \n",
      "                                                                 lstm_3[12][2]                    \n",
      "                                                                 dot_1[14][0]                     \n",
      "                                                                 lstm_3[13][0]                    \n",
      "                                                                 lstm_3[13][2]                    \n",
      "                                                                 dot_1[15][0]                     \n",
      "                                                                 lstm_3[14][0]                    \n",
      "                                                                 lstm_3[14][2]                    \n",
      "                                                                 dot_1[16][0]                     \n",
      "                                                                 lstm_3[15][0]                    \n",
      "                                                                 lstm_3[15][2]                    \n",
      "                                                                 dot_1[17][0]                     \n",
      "                                                                 lstm_3[16][0]                    \n",
      "                                                                 lstm_3[16][2]                    \n",
      "                                                                 dot_1[18][0]                     \n",
      "                                                                 lstm_3[17][0]                    \n",
      "                                                                 lstm_3[17][2]                    \n",
      "                                                                 dot_1[19][0]                     \n",
      "                                                                 lstm_3[18][0]                    \n",
      "                                                                 lstm_3[18][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "outputLayer (Dense)             (None, 4004)         260260      lstm_3[0][0]                     \n",
      "                                                                 lstm_3[1][0]                     \n",
      "                                                                 lstm_3[2][0]                     \n",
      "                                                                 lstm_3[3][0]                     \n",
      "                                                                 lstm_3[4][0]                     \n",
      "                                                                 lstm_3[5][0]                     \n",
      "                                                                 lstm_3[6][0]                     \n",
      "                                                                 lstm_3[7][0]                     \n",
      "                                                                 lstm_3[8][0]                     \n",
      "                                                                 lstm_3[9][0]                     \n",
      "                                                                 lstm_3[10][0]                    \n",
      "                                                                 lstm_3[11][0]                    \n",
      "                                                                 lstm_3[12][0]                    \n",
      "                                                                 lstm_3[13][0]                    \n",
      "                                                                 lstm_3[14][0]                    \n",
      "                                                                 lstm_3[15][0]                    \n",
      "                                                                 lstm_3[16][0]                    \n",
      "                                                                 lstm_3[17][0]                    \n",
      "                                                                 lstm_3[18][0]                    \n",
      "                                                                 lstm_3[19][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "convertSwapLayer3D (Lambda)     (None, 20, 4004)     0           outputLayer[0][0]                \n",
      "                                                                 outputLayer[1][0]                \n",
      "                                                                 outputLayer[2][0]                \n",
      "                                                                 outputLayer[3][0]                \n",
      "                                                                 outputLayer[4][0]                \n",
      "                                                                 outputLayer[5][0]                \n",
      "                                                                 outputLayer[6][0]                \n",
      "                                                                 outputLayer[7][0]                \n",
      "                                                                 outputLayer[8][0]                \n",
      "                                                                 outputLayer[9][0]                \n",
      "                                                                 outputLayer[10][0]               \n",
      "                                                                 outputLayer[11][0]               \n",
      "                                                                 outputLayer[12][0]               \n",
      "                                                                 outputLayer[13][0]               \n",
      "                                                                 outputLayer[14][0]               \n",
      "                                                                 outputLayer[15][0]               \n",
      "                                                                 outputLayer[16][0]               \n",
      "                                                                 outputLayer[17][0]               \n",
      "                                                                 outputLayer[18][0]               \n",
      "                                                                 outputLayer[19][0]               \n",
      "==================================================================================================\n",
      "Total params: 20,370,689\n",
      "Trainable params: 370,489\n",
      "Non-trainable params: 20,000,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:88: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "translator_v3 2019-05-15 17:10:25,092 [INFO] - <ipython-input-18-02b2c533e5d5>:90 srcLanguageModel embedding is loaded\n"
     ]
    }
   ],
   "source": [
    "T2 = Translator2(     \n",
    "                logger = None, \n",
    "    \n",
    "                modelName = 'translator_v3',    # v3 is the big release, in this version, the input and output is modificated of the attention-2 model\n",
    "                version = 2,\n",
    "                loadTrainedModel = False,     # load trained translator\n",
    "                checkpointDir = checkpointDir, \n",
    "\n",
    "                loadSrcLanguageModel = True,    # load source trained language model, e.g. glove\n",
    "                srcLanguageModelPath = enW2VPath,\n",
    "    \n",
    "                loadTarLanguageModel = False,    # load target trained language model, e.g. \n",
    "                tarLanguageModelPath = None,\n",
    "\n",
    "                senMaxLen = maxSenLen,\n",
    "                hiddenUnitNum = hiddenUnitNum,    # LSTM units\n",
    "                srcVocabularySize = None,    # source language embedding input size\n",
    "                tarVocabularySize = chVocabularySize,    # target language embedding input size, needed for output layer\n",
    "                srcEmbeddingSize = None,    # embedding output size\n",
    "                tarEmbeddingSize = None,\n",
    "                energySize = energySize,    # energy size of attention mechanism\n",
    "\n",
    "                droupOutRatio = 0.2, \n",
    "                batchSize = batchSize,    # from input dataset\n",
    "                epochNum = None,     # model.train\n",
    "                verbose = None,    # model.train\n",
    "                validationRatio = None,    # from validation dataset\n",
    "                patience = None    # model.train\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 20, 4004)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T2.model.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn():\n",
    "    return get_model_input(enPath + '_processed_index_train', chPath + '_processed_index_train', \n",
    "                           maxSenLen = maxSenLen, batchSize = batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_input_fn():\n",
    "    return get_model_input(enPath + '_processed_index_test', chPath + '_processed_index_test', \n",
    "                           maxSenLen = maxSenLen, batchSize = batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " - 52s - loss: 3.3233 - val_loss: 3.3275\n",
      "Epoch 2/20\n",
      " - 5s - loss: 3.2853 - val_loss: 3.2808\n",
      "Epoch 3/20\n",
      " - 4s - loss: 3.3820 - val_loss: 3.1704\n",
      "Epoch 4/20\n",
      " - 5s - loss: 3.0938 - val_loss: 3.3057\n",
      "Epoch 5/20\n",
      " - 5s - loss: 3.2666 - val_loss: 3.2113\n",
      "Epoch 6/20\n",
      " - 5s - loss: 3.2581 - val_loss: 3.2549\n",
      "Epoch 7/20\n",
      " - 5s - loss: 3.2587 - val_loss: 3.1145\n",
      "Epoch 8/20\n",
      " - 5s - loss: 3.2302 - val_loss: 3.1709\n",
      "Epoch 9/20\n",
      " - 5s - loss: 3.2539 - val_loss: 3.1278\n",
      "Epoch 10/20\n",
      " - 4s - loss: 3.2764 - val_loss: 3.2399\n",
      "Epoch 11/20\n",
      " - 4s - loss: 3.2149 - val_loss: 3.2068\n",
      "Epoch 12/20\n",
      " - 6s - loss: 3.0888 - val_loss: 3.1183\n",
      "Epoch 13/20\n",
      " - 4s - loss: 3.1632 - val_loss: 3.1865\n",
      "Epoch 14/20\n",
      " - 4s - loss: 3.1524 - val_loss: 3.2132\n",
      "Epoch 15/20\n",
      " - 5s - loss: 3.1458 - val_loss: 3.2328\n",
      "Epoch 16/20\n",
      " - 4s - loss: 3.2030 - val_loss: 3.0791\n",
      "Epoch 17/20\n",
      " - 5s - loss: 3.1372 - val_loss: 3.2280\n",
      "Epoch 18/20\n",
      " - 4s - loss: 3.2696 - val_loss: 3.0880\n",
      "Epoch 19/20\n",
      " - 4s - loss: 3.0762 - val_loss: 3.2784\n",
      "Epoch 20/20\n",
      " - 5s - loss: 3.1405 - val_loss: 3.1785\n"
     ]
    }
   ],
   "source": [
    "T2.train(train_input_fn = train_input_fn, val_input_fn = val_input_fn, epochs = 20, steps_per_epoch = 2, validation_steps = 2, verbose = 2) # stepsPerEpoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "pEn3 = Preprocessor(clean_en, tokenize_en, add_tag, skip_line, enVocabularySize)\n",
    "# pEn3.load_language_maps(enPath + '_idx2Lang')\n",
    "pEn3.load_language_maps(os.path.join(datasetPath, 'enW2v_idx2Lang'))\n",
    "\n",
    "pCh3 = Preprocessor()\n",
    "# pCh3.load_language_maps(os.path.join(datasetPath, 'chW2v_idx2Lang'))\n",
    "pCh3.load_language_maps(chPath + '_idx2Lang')\n",
    "\n",
    "senIdxExample = pEn3.index_raw_sentence('This is a really good day')\n",
    "senIdxPadExample = tf.keras.preprocessing.sequence.pad_sequences([senIdxExample], maxlen=maxSenLen, padding='post')\n",
    "# labelPadExample = tf.zeros((maxSenLen, chVocabularySize + 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputSenDistribution = T2.model.predict(x = [senIdxPadExample])[0]\n",
    "transIdxList = np.argmax(outputSenDistribution, axis = -1)\n",
    "print(transIdxList)\n",
    "pCh3.retrive_terms(transIdxList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strategy = tf.contrib.distribute.MirroredStrategy()\n",
    "# config = how (train_distribute = strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.estimator.RunConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()\n",
    "# T2_estimator = tf.keras.estimator.model_to_estimator(keras_model = T2.model)    # config = config, model_dir = checkpointDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# T2_estimator.train(input_fn = model_input_fn, steps = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1. Export model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/shared/workspace/Dev/translate/checkpoints/translator_v3/2'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_path = os.path.join(checkpointDir, T2.modelName, str(T2.version))    # ref: 17\n",
    "export_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-31-6c2415b8bcb7>:4: simple_save (from tensorflow.python.saved_model.simple_save) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.simple_save.\n",
      "WARNING:tensorflow:From /root/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:205: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /root/shared/workspace/Dev/translate/checkpoints/translator_v3/2/saved_model.pb\n",
      "total 14616\n",
      "-rwxrwxrwx 1 root root 14958933 May 15 17:15 saved_model.pb\n",
      "drwxrwxrwx 1 root root     4096 May 15 17:15 variables\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.simple_save(tf.keras.backend.get_session(),     # ref: 16\n",
    "                           export_path, \n",
    "                           inputs = {'sourceLanguageIdx': T2.model.input}, \n",
    "                           outputs = {'targetLanguageDistribution': T2.model.output})\n",
    "\n",
    "! ls -l {export_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['sourceLanguageIdx'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 20)\n",
      "        name: sourceLanguageIdx_1:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['targetLanguageDistribution'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 20, 4004)\n",
      "        name: convertSwapLayer3D_1/transpose:0\n",
      "  Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir {export_path} --all    # ref: 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2. Request the serving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 41, 18, 11, 592, 223, 126, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " senIdxPadExample.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"signature_name\": \"serving_default\", \"instances\": [[1, 41, 18, 11, 592, 223, 126, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}'\n"
     ]
    }
   ],
   "source": [
    "params = {\"signature_name\": \"serving_default\", \"instances\": senIdxPadExample.tolist()}\n",
    "data = json.dumps(params).encode('utf-8')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://192.168.31.101:8501/v1/models/translator_v3:predict'\n",
    "req = urllib.request.Request(url, data)\n",
    "opener = urllib.request.build_opener()\n",
    "response = opener.open(req).read()\n",
    "prediction = json.loads(response)['predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 20, 4004)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(prediction).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transIdxList = np.argmax(prediction, axis = -1)[0]\n",
    "print(transIdxList)\n",
    "pCh3.retrive_terms(transIdxList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "1. https://blog.csdn.net/m0_38007695/article/details/84723848\n",
    "2. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb\n",
    "3. https://www.coursera.org/learn/nlp-sequence-models/home/week/3\n",
    "4. https://blog.csdn.net/bmjhappy/article/details/80512917\n",
    "5. https://machinelearningmastery.com/return-sequences-and-return-states-for-lstms-in-keras/\n",
    "6. https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits_v2\n",
    "7. https://stackoverflow.com/questions/53580544/why-do-i-get-error-while-trying-to-build-an-architecture-with-multiple-inputs-in\n",
    "8. https://github.com/keras-team/keras/issues/6263\n",
    "9. https://www.tensorflow.org/guide/keras#input_tfdata_datasets\n",
    "10. https://stackoverflow.com/questions/50873615/how-to-obtain-the-runtime-batch-size-of-a-keras-model\n",
    "11. https://github.com/keras-team/keras/issues/4781\n",
    "12. https://github.com/keras-team/keras/issues/8343#issuecomment-385103183  \n",
    "13. https://github.com/CharlesWu123/SelfStudyTF/blob/master/TED_process/seq2seq_train.py  \n",
    "14. https://adamtiger.github.io/NNSharp/recurrents/  (kernel & recurrent kernel, recurrent activation & activation) \n",
    "15. https://blog.csdn.net/MyArrow/article/details/53445369  (orthagonal matrix)  \n",
    "16. https://colab.research.google.com/github/tensorflow/tfx/blob/master/docs/tutorials/serving/rest_simple.ipynb  \n",
    "17. https://stackoverflow.com/questions/45544928/tensorflow-serving-no-versions-of-servable-model-found-under-base-path  \n",
    "18. https://www.tensorflow.org/tfx/serving/api_rest  \n",
    "19. Shen Li, Zhe Zhao, Renfen Hu, Wensi Li, Tao Liu, Xiaoyong Du, Analogical Reasoning on Chinese Morphological and Semantic Relations, ACL 2018.\n",
    "20. https://github.com/Embedding/Chinese-Word-Vectors  \n",
    "21. https://github.com/tensorflow/serving/blob/master/tensorflow_serving/g3doc/serving_basic.md  \n",
    "22. https://stackoverflow.com/questions/41162876/get-weight-matrices-from-gensim-word2vec  \n",
    "23. https://stackoverflow.com/questions/53249304/how-to-get-the-list-all-existing-loggers-using-python-logging-module  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36] *",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
